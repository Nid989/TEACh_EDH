{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import yaml\n",
        "import pickle\n",
        "import warnings\n",
        "import logging\n",
        "import gc\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "from typing import Optional\n",
        "from datetime import datetime\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUOj5BKHjVGT",
        "outputId": "8e2b4928-c72f-4e40-ee89-4cb30c3d3890"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------- CONFIGURATIONS --------------------------------------------------------- #"
      ],
      "metadata": {
        "id": "x98jbUqao7R1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TARGET_COLUMN = \"code_mixed_explanation\"\n",
        "# TEXT_INPUT_PATH = \"../Data/Text/\"\n",
        "# ACOUSTIC_INPUT_PATH = \"../Data/Audio/\"\n",
        "# VISUAL_INPUT_PATH = \"../Data/Video/\"\n",
        "\n",
        "# MODEL_OUTPUT_DIR = check_and_create_directory(\"../models/MAF_TAV_BART/\")\n",
        "# RESULT_OUTPUT_DIR = check_and_create_directory(\"../results/MAF_TAV_BART/\")\n",
        "\n",
        "# LOWERCASE_UTTERANCES = False\n",
        "# UNFOLDED_DIALOGUE = True\n",
        "\n",
        "# if UNFOLDED_DIALOGUE:\n",
        "#     SOURCE_COLUMN = \"dialogue\"\n",
        "# else:\n",
        "#     SOURCE_COLUMN_1 = \"target\"\n",
        "#     SOURCE_COLUMN_2 = \"context\"\n",
        "\n",
        "SOURCE_MAX_LEN = 480\n",
        "TARGET_MAX_LEN = 50\n",
        "MAX_UTTERANCES = 25\n",
        "\n",
        "ACOUSTIC_DIM = 154\n",
        "ACOUSTIC_MAX_LEN = 600\n",
        "\n",
        "VISUAL_DIM = 2048\n",
        "VISUAL_MAX_LEN = 96\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "MAX_EPOCHS = 60\n",
        "\n",
        "BASE_LEARNING_RATE = 5e-6\n",
        "NEW_LEARNING_RATE = 5e-5\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "NUM_BEAMS = 5\n",
        "EARLY_STOPPING = True\n",
        "NO_REPEAT_NGRAM_SIZE = 3\n",
        "\n",
        "EARLY_STOPPING_THRESHOLD = 5"
      ],
      "metadata": {
        "id": "lDdGKxlfo6ye"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-54PhZyVdmTO"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------------- MODEL --------------------------------------------------------- #"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers datasets evaluate\n",
        "!pip install transformer-encoder"
      ],
      "metadata": {
        "id": "s-ihI80Th9Mw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
        "\n",
        "from transformers.modeling_utils import PreTrainedModel, unwrap_model\n",
        "\n",
        "from transformers import (\n",
        "    BartTokenizerFast,\n",
        "    AdamW\n",
        ")\n",
        "\n",
        "from transformers.models.bart.configuration_bart import BartConfig\n",
        "\n",
        "from transformers.models.bart.modeling_bart import (\n",
        "    BartPretrainedModel,\n",
        "    BartDecoder,\n",
        "    BartLearnedPositionalEmbedding,\n",
        "    BartEncoderLayer,\n",
        "    shift_tokens_right,\n",
        "    _make_causal_mask,\n",
        "    _expand_mask\n",
        ")\n",
        "\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutput,\n",
        "    Seq2SeqLMOutput,\n",
        "    Seq2SeqModelOutput\n",
        ")\n",
        "\n",
        "from transformer_encoder import TransformerEncoder"
      ],
      "metadata": {
        "id": "QMUKu8h1imSM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "Fm4vpJ6RlR0t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------- Mulitmodal Context Aware Attention --------------------------------------------------------- #"
      ],
      "metadata": {
        "id": "GsVIhtcHiDG4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextAwareAttention(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 dim_model: int,\n",
        "                 dim_context: int,\n",
        "                 dropout_rate: Optional[float]=0.0):\n",
        "        super(ContextAwareAttention, self).__init__()\n",
        "\n",
        "        self.dim_model = dim_model\n",
        "        self.dim_context = dim_context\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        self.attention_layer = nn.MultiheadAttention(embed_dim=self.dim_model,\n",
        "                                                     num_heads=1,\n",
        "                                                     dropout=self.dropout_rate,\n",
        "                                                     bias=True,\n",
        "                                                     add_zero_attn=False,\n",
        "                                                     batch_first=True,\n",
        "                                                     device=self.device)\n",
        "\n",
        "\n",
        "        self.u_k = nn.Linear(self.dim_context, self.dim_model, bias=False)\n",
        "        self.w1_k = nn.Linear(self.dim_model, 1, bias=False)\n",
        "        self.w2_k = nn.Linear(self.dim_model, 1, bias=False)\n",
        "\n",
        "        self.u_v = nn.Linear(self.dim_context, self.dim_model, bias=False)\n",
        "        self.w1_v = nn.Linear(self.dim_model, 1, bias=False)\n",
        "        self.w2_v = nn.Linear(self.dim_model, 1, bias=False)\n",
        "\n",
        "    def forward(self,\n",
        "                q: torch.Tensor,\n",
        "                k: torch.Tensor,\n",
        "                v: torch.Tensor,\n",
        "                context: Optional[torch.Tensor]=None):\n",
        "\n",
        "        key_context = self.u_k(context)\n",
        "        value_context = self.u_v(context)\n",
        "\n",
        "        lambda_k = F.sigmoid(self.w1_k(k) + self.w2_k(key_context))\n",
        "        lambda_v = F.sigmoid(self.w1_v(v) + self.w2_v(value_context))\n",
        "\n",
        "        k_cap = (1 - lambda_k) * k + lambda_k * key_context\n",
        "        v_cap = (1 - lambda_v) * v + lambda_v * value_context\n",
        "\n",
        "        attention_output, _ = self.attention_layer(query=q,\n",
        "                                                   key=k_cap,\n",
        "                                                   value=v_cap)\n",
        "        return attention_output"
      ],
      "metadata": {
        "id": "UIxZFsogiTnW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------- Modality Aware Fusion --------------------------------------------------------- #"
      ],
      "metadata": {
        "id": "6eeNGVkilDwG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MAF(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 dim_model: int,\n",
        "                 dropout_rate: int,\n",
        "                 util_config: dict):\n",
        "        super(MAF, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.action_context_transform = nn.Linear(util_config[\"SOURCE_ACTION_MAX_LEN\"],\n",
        "                                                    util_config[\"SOURCE_DIALOG_MAX_LEN\"], bias=False)\n",
        "        self.visual_context_transform = nn.Linear(util_config[\"SOURCE_VISUAL_MAX_LEN\"],\n",
        "                                                  util_config[\"SOURCE_DIALOG_MAX_LEN\"], bias=False)\n",
        "\n",
        "        self.action_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "                                                              dim_context=util_config[\"ACTION_DIM\"],\n",
        "                                                              dropout_rate=dropout_rate)\n",
        "        self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "                                                              dim_context=util_config[\"VISUAL_DIM\"],\n",
        "                                                              dropout_rate=dropout_rate)\n",
        "        self.action_gate = nn.Linear(2*dim_model, dim_model)\n",
        "        self.visual_gate = nn.Linear(2*dim_model, dim_model)\n",
        "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "        self.final_layer_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "    def forward(self,\n",
        "                text_input: torch.Tensor,\n",
        "                action_context: Optional[torch.Tensor]=None,\n",
        "                visual_context: Optional[torch.Tensor]=None):\n",
        "\n",
        "        # Audio as Context for Attention\n",
        "        action_context = action_context.permute(0, 2, 1)\n",
        "        action_context = self.action_context_transform(action_context)\n",
        "        action_context = action_context.permute(0, 2, 1)\n",
        "\n",
        "        # Video as Context for Attention\n",
        "        visual_context = visual_context.permute(0, 2, 1)\n",
        "        visual_context = self.visual_context_transform(visual_context)\n",
        "        visual_context = visual_context.permute(0, 2, 1)\n",
        "\n",
        "        action_out = self.action_context_attention(q=text_input,\n",
        "                                                    k=text_input,\n",
        "                                                    v=text_input,\n",
        "                                                    context=action_context)\n",
        "\n",
        "\n",
        "        video_out = self.visual_context_attention(q=text_input,\n",
        "                                                  k=text_input,\n",
        "                                                  v=text_input,\n",
        "                                                  context=visual_context)\n",
        "\n",
        "        # Global Information Fusion Mechanism\n",
        "        weight_a = F.sigmoid(self.action_gate(torch.cat((action_out, text_input), dim=-1)))\n",
        "        weight_v = F.sigmoid(self.visual_gate(torch.cat((video_out, text_input), dim=-1)))\n",
        "\n",
        "        output = self.final_layer_norm(text_input +\n",
        "                                       weight_a * action_out +\n",
        "                                       weight_v * video_out)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "BofVksfDlLko"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------- Multimodal BartEncoder --------------------------------------------------------- #"
      ],
      "metadata": {
        "id": "pxAbqmvkluv8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalBartEncoder(BartPretrainedModel):\n",
        "    \"\"\"\n",
        "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
        "    :class:`BartEncoderLayer`.\n",
        "    Args:\n",
        "        config: BartConfig\n",
        "        embed_tokens (nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: BartConfig, util_config: dict,\n",
        "                 embed_tokens: Optional[nn.Embedding] = None,\n",
        "                 embed_action_tokens: Optional[nn.Embedding] = None):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.encoder_layerdrop\n",
        "\n",
        "        embed_dim = config.d_model\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.max_source_positions = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n",
        "\n",
        "        if embed_action_tokens is not None:\n",
        "            self.embed_action_tokens = embed_action_tokens\n",
        "        else:\n",
        "            self.embed_action_tokens = nn.Embedding(util_config[\"ACTION_COUNT\"], util_config[\"ACTION_DIM\"], util_config[\"ACTION_PADDING_IDX\"])\n",
        "\n",
        "        self.embed_positions = BartLearnedPositionalEmbedding(\n",
        "            config.max_position_embeddings,\n",
        "            embed_dim,\n",
        "        )\n",
        "        self.layers = nn.ModuleList([BartEncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "        self.layernorm_embedding = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.init_weights()\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "        # ================================ Modifications ================================ #\n",
        "        # We prompt the Encoder to fuse the acoustic and visual features @ layer-4 of the BartEncoder model\n",
        "        # Also, we initialize two separate TransformerEncoder to encode the visual and acoustic information\n",
        "        # We also initialize the MAF for multimodal fusion.\n",
        "        self.fusion_at_layer = [4]\n",
        "        self.visual_transformer = TransformerEncoder(d_model=util_config[\"VISUAL_DIM\"],\n",
        "                                                     n_layers=4,\n",
        "                                                     n_heads=8,\n",
        "                                                     d_ff=util_config[\"VISUAL_DIM\"])\n",
        "        self.action_transformer = TransformerEncoder(d_model=util_config[\"ACTION_DIM\"],\n",
        "                                                     n_layers=4,\n",
        "                                                     n_heads=2,\n",
        "                                                     d_ff=util_config[\"ACTION_DIM\"])\n",
        "        self.MAF_layer = MAF(util_config=util_config,\n",
        "                             dim_model=embed_dim,\n",
        "                             dropout_rate=0.2)\n",
        "        # =============================================================================== #\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        action_input=None,      # New addition of action_input\n",
        "        visual_input=None,      # New addition of visual_input\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
        "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
        "                provide it.\n",
        "                Indices can be obtained using :class:`~transformers.BartTokenizer`. See\n",
        "                :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__`\n",
        "                for details.\n",
        "                `What are input IDs? <../glossary.html#input-ids>`__\n",
        "            attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "                Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "            head_mask (:obj:`torch.Tensor` of shape :obj:`(encoder_layers, encoder_attention_heads)`, `optional`):\n",
        "                Mask to nullify selected heads of the attention modules. Mask values selected in ``[0, 1]``:\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the head is **masked**.\n",
        "            inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "                Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded\n",
        "                representation. This is useful if you want more control over how to convert :obj:`input_ids` indices\n",
        "                into associated vectors than the model's internal embedding lookup matrix.\n",
        "            output_attentions (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
        "                returned tensors for more detail.\n",
        "            output_hidden_states (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
        "                for more detail.\n",
        "            return_dict (:obj:`bool`, `optional`):\n",
        "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # retrieve input_ids and inputs_embeds\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input = input_ids\n",
        "            input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input = inputs_embeds[:, :, -1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "\n",
        "        embed_pos = self.embed_positions(input)\n",
        "        embed_pos = embed_pos.to(inputs_embeds.device)\n",
        "\n",
        "        hidden_states = inputs_embeds + embed_pos\n",
        "        hidden_states = self.layernorm_embedding(hidden_states)\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "        # expand attention_mask\n",
        "        if attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)\n",
        "\n",
        "        encoder_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "\n",
        "        # check if head_mask has a correct number of layers specified if desired\n",
        "        if head_mask is not None:\n",
        "            assert head_mask.size()[0] == (\n",
        "                len(self.layers)\n",
        "            ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n",
        "\n",
        "        # ================================ Modifications ================================ #\n",
        "        action_input = self.embed_action_tokens(action_input.to(torch.int64))\n",
        "        # =============================================================================== #\n",
        "\n",
        "        for idx, encoder_layer in enumerate(self.layers):\n",
        "\n",
        "            # ================================ Modifications ================================ #\n",
        "            if idx in self.fusion_at_layer:\n",
        "                # modification compared to original implementation\n",
        "                # 1. adding extra 'mask' argument due to parameter compulsion\n",
        "                # 2. not mandatory to provide actual 'mask' value, set to 'None', instead of 'input.ne(0)' from TransformerEncoder implementation\n",
        "                # 3. replacing [-1] since the last element of batch is only selected\n",
        "\n",
        "                action_input = self.action_transformer(action_input, mask=None)\n",
        "                # acoustic_input = self.acoustic_transformer(acoustic_input, mask=None)[-1]\n",
        "                # acoustic_input = self.acoustic_transformer(acoustic_input, acoustic_input.ne(0))[-1]\n",
        "\n",
        "                visual_input = self.visual_transformer(visual_input, mask=None)\n",
        "                # visual_input = self.visual_transformer(visual_input, mask=None)[-1]\n",
        "                # visual_input = self.visual_transformer(visual_input, visual_input.ne(0))[-1]\n",
        "\n",
        "                hidden_states = self.MAF_layer(text_input=hidden_states,\n",
        "                                               action_context=action_input,\n",
        "                                               visual_context=visual_input)\n",
        "\n",
        "            # =============================================================================== #\n",
        "\n",
        "            if output_hidden_states:\n",
        "                encoder_states = encoder_states + (hidden_states,)\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
        "                layer_outputs = (None, None)\n",
        "            else:\n",
        "                if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                    def create_custom_forward(module):\n",
        "                        def custom_forward(*inputs):\n",
        "                            return module(*inputs, output_attentions)\n",
        "\n",
        "                        return custom_forward\n",
        "\n",
        "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                        create_custom_forward(encoder_layer),\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                        (head_mask[idx] if head_mask is not None else None),\n",
        "                    )\n",
        "                else:\n",
        "                    layer_outputs = encoder_layer(\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
        "                        output_attentions=output_attentions,\n",
        "                    )\n",
        "\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "\n",
        "            if output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            encoder_states = encoder_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
        "        )"
      ],
      "metadata": {
        "id": "FleqKc90l0g_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------- Mutlimodal ActionDecoder --------------------------------------------------------- #"
      ],
      "metadata": {
        "id": "sa7bsWpwsq5T"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalActionDecoder(nn.Module):\n",
        "    def __init__(self, config: dict, embed_action_tokens: Optional[nn.Embedding]=None):\n",
        "        super(MultimodalActionDecoder, self).__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.hidden_size = config[\"ACTION_DIM\"]\n",
        "        self.num_decoder_layers = config[\"NUM_DECODER_LAYERS\"]\n",
        "        self.dropout = nn.Dropout(config[\"DROPOUT\"])\n",
        "        self.padding_idx = config[\"ACTION_PADDING_IDX\"]\n",
        "\n",
        "        # FIXME batch_first seems to crash the system\n",
        "        self.transformer_decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(self.hidden_size, nhead=8, batch_first=True),\n",
        "            self.num_decoder_layers\n",
        "        )\n",
        "\n",
        "        self.embed_action_tokens = nn.Embedding(config[\"ACTION_COUNT\"], self.hidden_size, self.padding_idx)\n",
        "        if embed_action_tokens is not None:\n",
        "            self.embed_action_tokens.weight = embed_action_tokens.weight\n",
        "\n",
        "        self.fusion_layer = nn.Linear(self.hidden_size + config[\"VISUAL_DIM\"], self.hidden_size)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        decoder_action_input=None,\n",
        "        decoder_visual_input=None,\n",
        "        encoder_hidden_states=None,\n",
        "    ):\n",
        "        # embed actions\n",
        "        action_embeds = self.embed_action_tokens(decoder_action_input.to(torch.int64))\n",
        "        # fusion procedure; <actions, images> (representations)\n",
        "        fused_representation = self.fusion_layer(torch.cat((action_embeds, decoder_visual_input), dim=-1))\n",
        "        fused_representation = self.dropout(fused_representation)\n",
        "        # padded indices do not contribute to the gradient\n",
        "        padding_mask = (decoder_action_input == 0)\n",
        "\n",
        "        hidden_states = fused_representation\n",
        "        encoder_hidden_states = encoder_hidden_states\n",
        "        # NOTE: ideally the TransformerDecoder works w/ input dimensionality of\n",
        "        # sequence_length x batch_size x hidden_size but somehow the converse\n",
        "        # is working instead.\n",
        "        # hidden_states = fused_representation.permute(1, 0, 2)\n",
        "        # encoder_hidden_states = encoder_hidden_states.permute(1, 0, 2)\n",
        "\n",
        "        # decoder block\n",
        "        hidden_states = self.transformer_decoder(\n",
        "            tgt=hidden_states,\n",
        "            memory=encoder_hidden_states,\n",
        "            tgt_mask=None,\n",
        "            memory_mask=None,\n",
        "            tgt_key_padding_mask=None, # fix the permutation issue\n",
        "            memory_key_padding_mask=None\n",
        "        )\n",
        "\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states\n",
        "        )"
      ],
      "metadata": {
        "id": "t5u1TnOwso63"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------- Mutlimodal TEAChModel --------------------------------------------------------- #"
      ],
      "metadata": {
        "id": "mSK0MAdemAUT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalTEAChModel(BartPretrainedModel):\n",
        "    def __init__(self, config: BartConfig, util_config: dict):\n",
        "        super().__init__(config)\n",
        "\n",
        "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
        "        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n",
        "\n",
        "        # ================================ Modifications ================================ #\n",
        "        # We convert the unique action indices to a learned representation which will be further\n",
        "        # utilized alongwith visual and text modality features to retrieve final representation.\n",
        "        self.shared_action_embed = nn.Embedding(util_config[\"ACTION_COUNT\"], util_config[\"ACTION_DIM\"], util_config[\"ACTION_PADDING_IDX\"])\n",
        "        self.encoder = MultimodalBartEncoder(config, util_config, self.shared, self.shared_action_embed)\n",
        "        self.decoder = MultimodalActionDecoder(util_config, self.shared_action_embed)\n",
        "        # =============================================================================== #\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.shared\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.shared = value\n",
        "        self.encoder.embed_tokens = self.shared\n",
        "        self.decoder.embed_tokens = self.shared\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.encoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.decoder\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        action_input=None,      # New addition of acoustic_input\n",
        "        visual_input=None,      # New addition of visual_input\n",
        "        # decoder_input_ids=None,\n",
        "        # decoder_attention_mask=None,\n",
        "        decoder_action_input=None, # New addition of acoustic_input\n",
        "        decoder_visual_input=None, # New addition of acoustic_input\n",
        "        head_mask=None,\n",
        "        # decoder_head_mask=None,\n",
        "        # cross_attn_head_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        # past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        # decoder_inputs_embeds=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if encoder_outputs is None:\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                action_input=action_input,      # New addition of action_input\n",
        "                visual_input=visual_input,      # New addition of visual_input\n",
        "                head_mask=head_mask,\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
        "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "            encoder_outputs = BaseModelOutput(\n",
        "                last_hidden_state=encoder_outputs[0],\n",
        "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
        "            )\n",
        "\n",
        "        # ================================ Modifications ================================ #\n",
        "        decoder_outputs = self.decoder(\n",
        "            decoder_action_input=decoder_action_input,\n",
        "            decoder_visual_input=decoder_visual_input,\n",
        "            encoder_hidden_states=encoder_outputs[0]\n",
        "        )\n",
        "        # =============================================================================== #\n",
        "\n",
        "        if not return_dict:\n",
        "            return decoder_outputs + encoder_outputs\n",
        "\n",
        "        return Seq2SeqModelOutput(\n",
        "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
        "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "            encoder_attentions=encoder_outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "5VPZZpUDyweV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------- MultimodalTEAChModelForActionGeneration --------------------------------------------------------- #"
      ],
      "metadata": {
        "id": "ZacU5Tm_y1St"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalTEAChModelForActionGeneration(BartPretrainedModel):\n",
        "\n",
        "    def __init__(self, config: BartConfig, util_config: dict):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.config = config\n",
        "        self.util_config = util_config\n",
        "        self.model = MultimodalTEAChModel(config=config, util_config=util_config)\n",
        "        self.lm_head = nn.Linear(util_config[\"ACTION_DIM\"], util_config[\"ACTION_COUNT\"], bias=False)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        action_input=None,      # New addition of action_input\n",
        "        visual_input=None,      # New addition of visual_input\n",
        "        # decoder_input_ids=None,\n",
        "        # decoder_attention_mask=None,\n",
        "        decoder_action_input=None,      # New addition of decoder_action_input\n",
        "        decoder_visual_input=None,      # New addition of decoder_visual_input\n",
        "        head_mask=None,\n",
        "        # decoder_head_mask=None,\n",
        "        # cross_attn_head_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        # past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        # decoder_inputs_embeds=None,\n",
        "        labels=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the masked language modeling loss. Indices should either be in ``[0, ...,\n",
        "            config.vocab_size]`` or -100 (see ``input_ids`` docstring). Tokens with indices set to ``-100`` are ignored\n",
        "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``.\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "\n",
        "        if labels is not None:\n",
        "            if decoder_action_input is None:\n",
        "                decoder_action_input = shift_tokens_right(\n",
        "                    labels,\n",
        "                    self.util_config[\"ACTION_PADDING_IDX\"],\n",
        "                    self.util_config[\"ACTION_START_IDX\"]\n",
        "                )\n",
        "\n",
        "        # replace indices with pad_tokens in `labels` and set to -100\n",
        "        # since masked (-100) tokens are ignored while computing loss\n",
        "        labels[labels == self.util_config[\"ACTION_PADDING_IDX\"]] = -100\n",
        "        labels.to(torch.long)\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            action_input=action_input,      # New addition of action_input\n",
        "            visual_input=visual_input,      # New addition of visual_input\n",
        "            # decoder_input_ids=decoder_input_ids,\n",
        "            # decoder_attention_mask=decoder_attention_mask,\n",
        "            decoder_action_input=decoder_action_input,      # New addition of decoder_action_input\n",
        "            decoder_visual_input=decoder_visual_input,      # New addition of decoder_visual_input\n",
        "            head_mask=head_mask,\n",
        "            # decoder_head_mask=decoder_head_mask,\n",
        "            # cross_attn_head_mask=cross_attn_head_mask,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            # past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            # decoder_inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        lm_logits = self.lm_head(outputs[0])\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.util_config[\"ACTION_COUNT\"]), labels.view(-1).to(torch.int64))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (lm_logits,) + outputs[1:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return Seq2SeqLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=lm_logits,\n",
        "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
        "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
        "            encoder_attentions=outputs.encoder_attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "RvXYJTbxiGMF"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------- DATA UTILS --------------------------------------------------------- #"
      ],
      "metadata": {
        "id": "RZMo2d8rio1_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_from_yaml(path_to_file: str):\n",
        "    print(f\"loading data from .yaml file @ {path_to_file}\")\n",
        "    with open(path_to_file) as file:\n",
        "        _dict = yaml.safe_load(file)\n",
        "    return _dict\n",
        "\n",
        "def load_from_txt(path_to_file: str):\n",
        "    print(f\"loading data from .txt file @ {path_to_file}\")\n",
        "    with open (path_to_file, \"r\") as myfile:\n",
        "        data = myfile.read().splitlines()\n",
        "    return data\n",
        "\n",
        "def save_to_json(path_to_file: str, data: list):\n",
        "    with open(path_to_file, 'w') as outfile:\n",
        "        json.dump(data, outfile)\n",
        "    print(f\"file saved @ loc: {path_to_file}\")\n",
        "\n",
        "def load_from_json(path_to_file: str):\n",
        "    print(f\"loading data from .json file @ {path_to_file}\")\n",
        "    with open(path_to_file, \"r\") as json_file:\n",
        "        _dict = json.load(json_file)\n",
        "    return _dict\n",
        "\n",
        "def save_to_pickle(data_list, path_to_file):\n",
        "    with open(path_to_file, 'wb') as file:\n",
        "        pickle.dump(data_list, file)\n",
        "    print(f\"file saved @ loc: {path_to_file}\")\n",
        "\n",
        "def load_from_pickle(path_to_file):\n",
        "    print(f\"loading data from .pkl file @ {path_to_file}\")\n",
        "    with open(path_to_file, 'rb') as file:\n",
        "        data_list = pickle.load(file)\n",
        "    return data_list"
      ],
      "metadata": {
        "id": "rXWVZTQ4i404"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_seq(tensor: torch.tensor,\n",
        "            dim: int,\n",
        "            max_len: int,\n",
        "            pad_token_id: int=0):\n",
        "    if max_len > tensor.shape[0]:\n",
        "        return torch.cat([tensor, torch.ones(max_len - tensor.shape[0], dim) * pad_token_id])\n",
        "    else:\n",
        "        return tensor[:max_len]"
      ],
      "metadata": {
        "id": "ucjJCHMEgXdU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "Jx1DoNr6lH2e"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TEACh_EDH_Dataset:\n",
        "    def __init__(self, config, tokenizer):\n",
        "        super(TEACh_EDH_Dataset, self).__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.dataset = load_from_pickle(self.config[\"PATH_TO_DATA\"])\n",
        "        self.set_up_data_loader() # convert to tensors & batch dataset items\n",
        "\n",
        "    def preprocess_dataset(self):\n",
        "        source_dialogs = [item for item in self.dataset[self.config[\"SOURCE_DIALOG_COLUMN\"]]]\n",
        "        model_inputs = self.tokenizer(source_dialogs,\n",
        "                                max_length=self.config[\"SOURCE_DIALOG_MAX_LEN\"],\n",
        "                                padding=\"max_length\",\n",
        "                                truncation=True)\n",
        "        model_inputs['input_ids'] = torch.tensor([item for item in model_inputs['input_ids']], dtype=torch.long, device=self.device)\n",
        "        model_inputs['attention_mask'] = torch.tensor([item for item in model_inputs['attention_mask']], dtype=torch.long, device=self.device)\n",
        "        # pad action sequences i.e. source_actions & target_actions\n",
        "        # NOTE: avoiding the processing of obj_interaction_actions and objects\n",
        "        # source_actions\n",
        "        model_inputs[\"action_input\"] = torch.stack([pad_seq(torch.tensor(item, dtype=torch.int64).unsqueeze(dim=1),\n",
        "                                                            dim=1,\n",
        "                                                            max_len=self.config[\"SOURCE_ACTION_MAX_LEN\"])\n",
        "                                                    for item in self.dataset[self.config[\"SOURCE_ACTION_COLUMN\"]].values.tolist()], 0).to(self.device)\n",
        "        # target_actions (`labels`)\n",
        "        model_inputs[\"labels\"] = torch.stack([pad_seq(torch.tensor(item, dtype=torch.int64).unsqueeze(dim=1),\n",
        "                                                      dim=1,\n",
        "                                                      max_len=self.config[\"TARGET_ACTION_MAX_LEN\"],\n",
        "                                                      pad_token_id=self.config[\"ACTION_PADDING_IDX\"])\n",
        "                                              for item in self.dataset[self.config[\"TARGET_ACTION_COLUMN\"]].values.tolist()], 0).to(self.device)\n",
        "\n",
        "        # pad visual feats i.e. driver_images_history_feats & driver_images_future_feats\n",
        "        # driver_images_history_feats\n",
        "        model_inputs[\"visual_input\"] = torch.stack([pad_seq(torch.tensor(item, dtype=torch.float),\n",
        "                                                            dim=self.config[\"VISUAL_DIM\"],\n",
        "                                                            max_len=self.config[\"SOURCE_VISUAL_MAX_LEN\"])\n",
        "                                                    for item in self.dataset[self.config[\"SOURCE_VISUAL_COLUMN\"]].values.tolist()], 0).to(self.device)\n",
        "        # driver_images_future_feats\n",
        "        model_inputs[\"decoder_visual_input\"] = torch.stack([pad_seq(torch.tensor(item, dtype=torch.float),\n",
        "                                                            dim=self.config[\"VISUAL_DIM\"],\n",
        "                                                            max_len=self.config[\"TARGET_VISUAL_MAX_LEN\"])\n",
        "                                                    for item in self.dataset[self.config[\"TARGET_VISUAL_COLUMN\"]].values.tolist()], 0).to(self.device)\n",
        "\n",
        "        del source_dialogs\n",
        "        gc.collect()\n",
        "        return model_inputs\n",
        "\n",
        "    def set_up_data_loader(self):\n",
        "        dataset = self.preprocess_dataset()\n",
        "        print(dataset.keys())\n",
        "        dataset = TensorDataset(\n",
        "            dataset[\"input_ids\"],\n",
        "            dataset[\"attention_mask\"],\n",
        "            dataset[\"action_input\"].squeeze(-1),\n",
        "            dataset[\"visual_input\"],\n",
        "            dataset[\"decoder_visual_input\"],\n",
        "            dataset[\"labels\"].squeeze(-1),\n",
        "        )\n",
        "        self.data_loader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.config[\"BATCH_SIZE\"],\n",
        "            shuffle=True\n",
        "        )\n",
        "        del dataset\n",
        "        gc.collect()"
      ],
      "metadata": {
        "id": "uIsrwAkbn_Mj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = load_from_yaml(\"/content/config.yaml\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QFVgVQx25i3",
        "outputId": "b6faba58-10a6-4ebe-8678-df629ee849be"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading data from .yaml file @ /content/config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "TOKENIZER = BartTokenizerFast.from_pretrained(\"facebook/bart-large\")\n",
        "teach_edh_dataset = TEACh_EDH_Dataset(config, TOKENIZER)"
      ],
      "metadata": {
        "id": "UnOWFXuvgXUJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(teach_edh_dataset.data_loader))"
      ],
      "metadata": {
        "id": "c8AOja1XFbHQ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tuple(t.to(DEVICE) for t in batch)\n",
        "input_ids, attention_mask, action_input, visual_input, decoder_visual_input, labels = batch"
      ],
      "metadata": {
        "id": "x2UIGTd4WtEZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "Zm-vh7B2zb2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from transformers import BartModel\n",
        "bart_config = BartConfig.from_pretrained(config[\"MODEL_CHECKPOINT\"])"
      ],
      "metadata": {
        "id": "_MbzkGzMStvb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bart_model = BartModel.from_pretrained(config[\"MODEL_CHECKPOINT\"])\n",
        "# bart_encoder = MultimodalBartEncoder(config=bart_config, util_config=config, embed_tokens=bart_model.shared)\n",
        "# output = bart_encoder(input_ids=input_ids, attention_mask=attention_mask, action_input=action_input, visual_input=visual_input, output_hidden_states=True)"
      ],
      "metadata": {
        "id": "6VBgJmYVzazm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, attention_mask, action_input, visual_input, decoder_visual_input, labels = batch"
      ],
      "metadata": {
        "id": "ioadkB2Itoe2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "LJ2cxjTHxIV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# also need to consider how to provide the decoder_input_ids\n",
        "# correct the action vocabulary so that it can have initiall token-ids for <pad> <eos> etc.\n",
        "# also to the target action_output add <eoa> (end of action) as the final token and stop the output"
      ],
      "metadata": {
        "id": "li62c_1cWTDi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teach_model = MultimodalTEAChModelForActionGeneration(config=bart_config, util_config=config)"
      ],
      "metadata": {
        "id": "f-bY7ZPioMwn"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teach_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwxHuDl_adbR",
        "outputId": "d92f25ee-6cf6-492f-edcb-6b7fa69d192b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultimodalTEAChModelForActionGeneration(\n",
              "  (model): MultimodalTEAChModel(\n",
              "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
              "    (shared_action_embed): Embedding(19, 1024, padding_idx=0)\n",
              "    (encoder): MultimodalBartEncoder(\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_action_tokens): Embedding(19, 1024, padding_idx=0)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (visual_transformer): TransformerEncoder(\n",
              "        (multi_headed_attention): MultiHeadAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0-3): 4 x Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sdpa): ScaledDotProductAttention()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): FeedForward(\n",
              "          (w_1): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (w_2): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_layer): EncoderLayer(\n",
              "          (self_attn): MultiHeadAttention(\n",
              "            (linears): ModuleList(\n",
              "              (0-3): 4 x Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (sdpa): ScaledDotProductAttention()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (feed_forward): FeedForward(\n",
              "            (w_1): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (w_2): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (sublayer): ModuleList(\n",
              "            (0-1): 2 x SublayerConnection(\n",
              "              (norm): LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (encoder): Encoder(\n",
              "          (layers): ModuleList(\n",
              "            (0-3): 4 x EncoderLayer(\n",
              "              (self_attn): MultiHeadAttention(\n",
              "                (linears): ModuleList(\n",
              "                  (0-3): 4 x Linear(in_features=768, out_features=768, bias=True)\n",
              "                )\n",
              "                (sdpa): ScaledDotProductAttention()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (feed_forward): FeedForward(\n",
              "                (w_1): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (w_2): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (sublayer): ModuleList(\n",
              "                (0-1): 2 x SublayerConnection(\n",
              "                  (norm): LayerNorm()\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm()\n",
              "        )\n",
              "      )\n",
              "      (action_transformer): TransformerEncoder(\n",
              "        (multi_headed_attention): MultiHeadAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0-3): 4 x Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (sdpa): ScaledDotProductAttention()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): FeedForward(\n",
              "          (w_1): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (w_2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_layer): EncoderLayer(\n",
              "          (self_attn): MultiHeadAttention(\n",
              "            (linears): ModuleList(\n",
              "              (0-3): 4 x Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (sdpa): ScaledDotProductAttention()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (feed_forward): FeedForward(\n",
              "            (w_1): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (w_2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (sublayer): ModuleList(\n",
              "            (0-1): 2 x SublayerConnection(\n",
              "              (norm): LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (encoder): Encoder(\n",
              "          (layers): ModuleList(\n",
              "            (0-3): 4 x EncoderLayer(\n",
              "              (self_attn): MultiHeadAttention(\n",
              "                (linears): ModuleList(\n",
              "                  (0-3): 4 x Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                )\n",
              "                (sdpa): ScaledDotProductAttention()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (feed_forward): FeedForward(\n",
              "                (w_1): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (w_2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (sublayer): ModuleList(\n",
              "                (0-1): 2 x SublayerConnection(\n",
              "                  (norm): LayerNorm()\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (norm): LayerNorm()\n",
              "        )\n",
              "      )\n",
              "      (MAF_layer): MAF(\n",
              "        (action_context_transform): Linear(in_features=150, out_features=512, bias=False)\n",
              "        (visual_context_transform): Linear(in_features=150, out_features=512, bias=False)\n",
              "        (action_context_attention): ContextAwareAttention(\n",
              "          (attention_layer): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (u_k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "          (w1_k): Linear(in_features=1024, out_features=1, bias=False)\n",
              "          (w2_k): Linear(in_features=1024, out_features=1, bias=False)\n",
              "          (u_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "          (w1_v): Linear(in_features=1024, out_features=1, bias=False)\n",
              "          (w2_v): Linear(in_features=1024, out_features=1, bias=False)\n",
              "        )\n",
              "        (visual_context_attention): ContextAwareAttention(\n",
              "          (attention_layer): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (u_k): Linear(in_features=768, out_features=1024, bias=False)\n",
              "          (w1_k): Linear(in_features=1024, out_features=1, bias=False)\n",
              "          (w2_k): Linear(in_features=1024, out_features=1, bias=False)\n",
              "          (u_v): Linear(in_features=768, out_features=1024, bias=False)\n",
              "          (w1_v): Linear(in_features=1024, out_features=1, bias=False)\n",
              "          (w2_v): Linear(in_features=1024, out_features=1, bias=False)\n",
              "        )\n",
              "        (action_gate): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "        (visual_gate): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "        (dropout_layer): Dropout(p=0.2, inplace=False)\n",
              "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (decoder): MultimodalActionDecoder(\n",
              "      (dropout): Dropout(p=0.2, inplace=False)\n",
              "      (transformer_decoder): TransformerDecoder(\n",
              "        (layers): ModuleList(\n",
              "          (0-5): 6 x TransformerDecoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (multihead_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (linear2): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout1): Dropout(p=0.1, inplace=False)\n",
              "            (dropout2): Dropout(p=0.1, inplace=False)\n",
              "            (dropout3): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (embed_action_tokens): Embedding(19, 1024, padding_idx=0)\n",
              "      (fusion_layer): Linear(in_features=1792, out_features=1024, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=19, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = teach_model(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    action_input=action_input,\n",
        "    visual_input=visual_input,\n",
        "    decoder_visual_input=decoder_visual_input,\n",
        "    labels=labels,\n",
        ")"
      ],
      "metadata": {
        "id": "2539FzGEtD4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d5d0705-49eb-49e4-d8a7-90d62e7a5e18"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2548, -1.2251,  0.5783,  ...,  0.6186, -0.4218, -0.2350],\n",
            "        [ 0.5794, -1.0559,  0.4120,  ...,  0.8792, -0.5495,  0.2840],\n",
            "        [ 0.1059, -1.3615,  0.0887,  ...,  0.1632, -0.4649,  0.3318],\n",
            "        ...,\n",
            "        [-0.4314, -0.4185, -0.6107,  ...,  1.1773,  0.2687, -0.3284],\n",
            "        [-0.8682, -0.6672, -0.7186,  ...,  0.6177,  0.2013,  0.0569],\n",
            "        [-0.5996, -1.0394, -0.6842,  ...,  0.9173,  0.0211,  0.1168]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "-----------------------\n",
            "tensor([  11., -100., -100.,  ..., -100., -100., -100.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OooikblScifo",
        "outputId": "b40c2afa-b852-4073-9b2b-ecefd3392a28"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.7808, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R3uoASUndhFJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}